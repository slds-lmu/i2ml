---
title: "Tuning: Further Reading"
weight: 8006
---

<!--more-->

#### Literature cited in this chapter

{{< pdfjs file="https://github.com/slds-lmu/i2ml/blob/hpo-link/content/appendix/references.pdf" >}}

#### Further Material

- Bischl, Bernd, et al. ["Hyperparameter optimization: Foundations, algorithms, best practices, and open challenges."](https://wires.onlinelibrary.wiley.com/doi/full/10.1002/widm.1484) Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery (2021): e1484.  
    This paper goes beyond grid search and random search and reviews important automatic hyperparameter optimization (HPO) methods, provides practical recommendations for conducting HPO, and discusses HPO algorithms, performance evaluation, combination with machine learning pipelines, runtime improvements, and parallelization.  
- mlr3 Practical Tuning Series:
    - [Part I - Tune a Support Vector Machine](https://mlr-org.com/gallery/series/2021-03-09-practical-tuning-series-tune-a-support-vector-machine/) 
    - [Part II - Tune a Preprocessing Pipeline](https://mlr-org.com/gallery/series/2021-03-10-practical-tuning-series-tune-a-preprocessing-pipeline/)
    - [Part III - Build an Automated Machine Learning System](https://mlr-org.com/gallery/series/2021-03-11-practical-tuning-series-build-an-automated-machine-learning-system/)
    - [Part IV - Tuning and Parallel Processing](https://mlr-org.com/gallery/series/2021-03-12-practical-tuning-series-tuning-and-parallel-processing/)
