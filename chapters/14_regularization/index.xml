<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Chapter 14: Regularization on Introduction to Machine Learning (I2ML)</title><link>https://slds-lmu.github.io/i2ml/chapters/14_regularization/</link><description>Recent content in Chapter 14: Regularization on Introduction to Machine Learning (I2ML)</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://slds-lmu.github.io/i2ml/chapters/14_regularization/index.xml" rel="self" type="application/rss+xml"/><item><title>Chapter 14.01: Introduction to Regularization</title><link>https://slds-lmu.github.io/i2ml/chapters/14_regularization/14-01-regu-intro/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/14_regularization/14-01-regu-intro/</guid><description>&lt;p>In this section, we revisit overfitting and introduce regularization as a remedy.&lt;/p></description></item><item><title>Chapter 14.02: Lasso and Ridge Regression</title><link>https://slds-lmu.github.io/i2ml/chapters/14_regularization/14-02-l1l2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/14_regularization/14-02-l1l2/</guid><description>&lt;p>We introduce Lasso and Ridge regression as the key approaches to regularizing linear models.&lt;/p></description></item><item><title>Chapter 14.03: Lasso vs Ridge Regression</title><link>https://slds-lmu.github.io/i2ml/chapters/14_regularization/14-03-l1vsl12/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/14_regularization/14-03-l1vsl12/</guid><description>&lt;p>This section provides a detailed comparison between Lasso and Ridge regression.&lt;/p></description></item><item><title>Chapter 14.04: Elastic Net and Regularization for GLMs</title><link>https://slds-lmu.github.io/i2ml/chapters/14_regularization/14-04-enetlogreg/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/14_regularization/14-04-enetlogreg/</guid><description>&lt;p>In this section, we introduce the elastic net as a combination of Ridge and Lasso regression and discuss regularization for logistic regression.&lt;/p></description></item><item><title>Chapter 14.05: Regularization for Underdetermined Problems</title><link>https://slds-lmu.github.io/i2ml/chapters/14_regularization/14-05-underdetermined/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/14_regularization/14-05-underdetermined/</guid><description>&lt;p>In this section, we discuss how regularization can make ill-posed problems well-defined.&lt;/p></description></item><item><title>Chapter 14.06: L0 Regularization</title><link>https://slds-lmu.github.io/i2ml/chapters/14_regularization/14-06-l0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/14_regularization/14-06-l0/</guid><description>&lt;p>In this section, we introduce \(LQ\) regularization and particularly discuss \(L0\) regularization as an important special case besides \(L1\) and \(L2\) that penalizes the number of non-zero parameters.&lt;/p></description></item><item><title>Chapter 14.07: Regularization in NonLinear Models and Bayesian Priors</title><link>https://slds-lmu.github.io/i2ml/chapters/14_regularization/14-07-nonlin-bayes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/14_regularization/14-07-nonlin-bayes/</guid><description>&lt;p>In this section, we motivate regularization from a Bayesian perspective, showing how different penalty terms correspond to different Bayesian priors.&lt;/p></description></item><item><title>Chapter 14.08: Geometric Analysis of L2 Regularization and Weight Decay</title><link>https://slds-lmu.github.io/i2ml/chapters/14_regularization/14-08-geom-l2-wdecay/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/14_regularization/14-08-geom-l2-wdecay/</guid><description>&lt;p>In this section, we provide a geometric understanding of \(L2\) regularization, showing how parameters are shrunk according to the eigenvalues of the Hessian of empirical risk, and discuss its correspondence to weight decay.&lt;/p></description></item><item><title>Chapter 14.09: Geometric Analysis of L1 Regularization</title><link>https://slds-lmu.github.io/i2ml/chapters/14_regularization/14-09-geom-l1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/14_regularization/14-09-geom-l1/</guid><description>&lt;p>In this section, we provide a geometric understanding of \(L1\) regularization and show that it encourages sparsity in the parameter vector.&lt;/p></description></item><item><title>Chapter 14.10: Early Stopping</title><link>https://slds-lmu.github.io/i2ml/chapters/14_regularization/14-10-early-stopping/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/14_regularization/14-10-early-stopping/</guid><description>&lt;p>In this section, we introduce early stopping and show how it can act as a regularizer.&lt;/p></description></item></channel></rss>