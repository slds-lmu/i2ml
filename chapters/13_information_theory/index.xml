<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Chapter 13: Information Theory on Introduction to Machine Learning (I2ML)</title><link>https://slds-lmu.github.io/i2ml/chapters/13_information_theory/</link><description>Recent content in Chapter 13: Information Theory on Introduction to Machine Learning (I2ML)</description><generator>Hugo</generator><language>en-us</language><atom:link href="https://slds-lmu.github.io/i2ml/chapters/13_information_theory/index.xml" rel="self" type="application/rss+xml"/><item><title>Chapter 13.01: Entropy I</title><link>https://slds-lmu.github.io/i2ml/chapters/13_information_theory/13-01-entropy/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/13_information_theory/13-01-entropy/</guid><description>&lt;p>We introduce entropy, which expresses the expected information for discrete random variables, as a central concept in information theory.&lt;/p></description></item><item><title>Chapter 13.02: Entropy II</title><link>https://slds-lmu.github.io/i2ml/chapters/13_information_theory/13-02-entropy2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/13_information_theory/13-02-entropy2/</guid><description>&lt;p>We continue our discussion about entropy and introduce joint entropy, the uniqueness theorem and the maximum entropy principle.&lt;/p></description></item><item><title>Chapter 13.03: Differential Entropy</title><link>https://slds-lmu.github.io/i2ml/chapters/13_information_theory/13-03-diffent/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/13_information_theory/13-03-diffent/</guid><description>&lt;p>In this section, we extend the definition of entropy to the continuous case.&lt;/p></description></item><item><title>Chapter 13.04: Kullback-Leibler Divergence</title><link>https://slds-lmu.github.io/i2ml/chapters/13_information_theory/13-04-kl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/13_information_theory/13-04-kl/</guid><description>&lt;p>The Kullback-Leibler divergence (KL) is an important quantity for measuring the difference between two probability distributions. We discuss different intuitions for KL and relate it to risk minimization and likelihood ratios.&lt;/p></description></item><item><title>Chapter 13.05: Cross-Entropy and KL</title><link>https://slds-lmu.github.io/i2ml/chapters/13_information_theory/13-05-cross-entropy-kld/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/13_information_theory/13-05-cross-entropy-kld/</guid><description>&lt;p>We introduce cross-entropy as a further information-theoretic concept and discuss the connection between entropy, cross-entropy, and Kullback-Leibler divergence.&lt;/p></description></item><item><title>Chapter 13.06: Information Theory for Machine Learning</title><link>https://slds-lmu.github.io/i2ml/chapters/13_information_theory/13-06-ml/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/13_information_theory/13-06-ml/</guid><description>&lt;p>In this section, we discuss how information-theoretic concepts are used in machine learning and demonstrate the equivalence of KL minimization and maximum likelihood maximization, as well as how (cross-)entropy can be used as a loss function.&lt;/p></description></item><item><title>Chapter 13.07: Joint Entropy and Mutual Information I</title><link>https://slds-lmu.github.io/i2ml/chapters/13_information_theory/13-07-mutual-info/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/13_information_theory/13-07-mutual-info/</guid><description>&lt;p>Information theory also provides means of quantifying relations between two random variables that extend the concept of (linear) correlation. We discuss joint entropy, conditional entropy, and mutual information in this context.&lt;/p></description></item><item><title>Chapter 13.08: Joint Entropy and Mutual Information II</title><link>https://slds-lmu.github.io/i2ml/chapters/13_information_theory/13-08-mutual-info2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/13_information_theory/13-08-mutual-info2/</guid><description>&lt;p>Information theory also provides means of quantifying relations between two random variables that extend the concept of (linear) correlation. We discuss joint entropy, conditional entropy, and mutual information in this context.&lt;/p></description></item><item><title>Chapter 13.09: Entropy and Optimal Code Length I</title><link>https://slds-lmu.github.io/i2ml/chapters/13_information_theory/13-09-sourcecoding/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/13_information_theory/13-09-sourcecoding/</guid><description>&lt;p>In this section, we introduce source coding and discuss how entropy can be understood as optimal code length.&lt;/p></description></item><item><title>Chapter 13.10: Entropy and Optimal Code Length II</title><link>https://slds-lmu.github.io/i2ml/chapters/13_information_theory/13-10-sourcecoding2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/13_information_theory/13-10-sourcecoding2/</guid><description>&lt;p>In this section, we continue our discussion on source coding and its relation to entropy.&lt;/p></description></item><item><title>Chapter 13.11: MI under Reparametrization: Deep Dive</title><link>https://slds-lmu.github.io/i2ml/chapters/13_information_theory/13-11-mi-deepdive/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/13_information_theory/13-11-mi-deepdive/</guid><description>&lt;p>In this deep dive, we discuss the invariance of MI under certain reparametrizations.&lt;/p></description></item><item><title>Chapter Literature</title><link>https://slds-lmu.github.io/i2ml/chapters/13_information_theory/13-12-literature/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/13_information_theory/13-12-literature/</guid><description>&lt;script src="https://cdn.jsdelivr.net/npm/pdfjs-dist@2.9.359/build/pdf.min.js" integrity="sha256-hEmjt7z3bB53X/awJyV81gmBLpVw2mj7EsvoJelZWow=" crossorigin="anonymous">&lt;/script>




 
 
 &lt;a href="https://github.com/slds-lmu/lecture_sl/raw/main/slides-pdf/chapter-literature-information-theory.pdf">
 &lt;button class="btn btn-primary" style="margin-bottom:3rem">
 Download &amp;raquo;chapter-literature-information-theory.pdf&amp;laquo;
 &lt;/button>
 &lt;/a></description></item></channel></rss>