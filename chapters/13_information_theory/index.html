<!DOCTYPE html>
<html><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="/website_i2ml_copy/css/style.css">


<title>Introduction to Machine Learning (I2ML) | Chapter 13: Information Theory</title>


<link rel="apple-touch-icon" sizes="180x180" href="/website_i2ml_copy/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/website_i2ml_copy/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/website_i2ml_copy/favicon-16x16.png">
<link rel="manifest" href="/website_i2ml_copy/site.webmanifest">
<link rel="mask-icon" href="/website_i2ml_copy/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">

</head><body>
<img id="logo" src="/website_i2ml_copy/i2ml.svg" />

<div id="nav-border" class="container">
    <nav id="nav" class="nav justify-content-center">
        
        <a class="nav-link" href="/website_i2ml_copy">
        
        Home
        </a>
        
        <a class="nav-link" href="/website_i2ml_copy/chapters/">
        
        Chapters
        </a>
        
        <a class="nav-link" href="/website_i2ml_copy/appendix/">
        
        Appendix
        </a>
        
        <a class="nav-link" href="/website_i2ml_copy/exercises/">
        
        Exercises
        </a>
        
        <a class="nav-link" href="/website_i2ml_copy/references/">
        
        References
        </a>
        
        <a class="nav-link" href="/website_i2ml_copy/team/">
        
        Team
        </a>
        
    </nav>
</div><div id="content" class="container">
<h1>Chapter 13: Information Theory</h1>

<p><p>This chapter covers basic information-theoretic concepts and discusses their relation to machine learning.</p>
</p>


<div class="chapter_overview">
<ul class="list-unstyled">


<li>
    <a class="title" href="/website_i2ml_copy/chapters/13_information_theory/13-01-entropy/">Chapter 13.01: Entropy</a>
    
      
        <p>We introduce entropy, which expresses the expected information for discrete random variables, as a central concept in information theory.
</p>
      
      
</li>

<li>
    <a class="title" href="/website_i2ml_copy/chapters/13_information_theory/13-02-diffent/">Chapter 13.02: Differential Entropy</a>
    
      
        <p>In this section, we extend the definition of entropy to the continuous case.
</p>
      
      
</li>

<li>
    <a class="title" href="/website_i2ml_copy/chapters/13_information_theory/13-03-kl/">Chapter 13.03: Kullback-Leibler Divergence</a>
    
      
        <p>The Kullback-Leibler divergence (KL) is an important quantity for measuring the difference between two probability distributions. We discuss different intuitions for KL and relate it to risk minimization and likelihood ratios.
</p>
      
      
</li>

<li>
    <a class="title" href="/website_i2ml_copy/chapters/13_information_theory/13-04-sourcecoding/">Chapter 13.04: Entropy and Optimal Code Length</a>
    
      
        <p>In this section, we introduce source coding and discuss how entropy can be understood as optimal code length.
</p>
      
      
</li>

<li>
    <a class="title" href="/website_i2ml_copy/chapters/13_information_theory/13-05-cross-entropy-kld/">Chapter 13.05: Cross-Entropy, KL and Source Coding</a>
    
      
        <p>We introduce cross-entropy as a further information-theoretic concept and discuss the connection between entropy, cross-entropy, and Kullback-Leibler divergence.
</p>
      
      
</li>

<li>
    <a class="title" href="/website_i2ml_copy/chapters/13_information_theory/13-06-ml/">Chapter 13.06: Information Theory for Machine Learning</a>
    
      
        <p>In this section, we discuss how information-theoretic concepts are used in machine learning and demonstrate the equivalence of KL minimization and maximum likelihood maximization, as well as how (cross-)entropy can be used as a loss function.
</p>
      
      
</li>

<li>
    <a class="title" href="/website_i2ml_copy/chapters/13_information_theory/13-07-mutual-info/">Chapter 13.07: Joint Entropy and Mutual Information</a>
    
      
        <p>Information theory also provides means of quantifying relations between two random variables that extend the concept of (linear) correlation. We discuss joint entropy, conditional entropy, and mutual information in this context.
</p>
      
      
</li>


</ul>
</div>


        </div><footer class="bg-light text-center text-lg-start fixed-bottom">
<ul class="list-inline text-center">
  <li class="list-inline-item">Â© 2021 Course Creator</li>
  
  <li class="list-inline-item"><a class="nav-link" href="https://github.com/slds-lmu/lecture_i2ml" target="_blank">Course content</a></li>
  
  <li class="list-inline-item"><a class="nav-link" href="/website_i2ml_copy" target="_blank">Main Course Website</a></li>
  
  <li class="list-inline-item"><a class="nav-link" href="https://github.com/slds-lmu/i2ml" target="_blank">Website source code</a></li>
  
</ul>
</footer>



</body>
</html>
