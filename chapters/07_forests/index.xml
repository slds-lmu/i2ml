<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Chapter 07: Random Forests on Introduction to Machine Learning (I2ML)</title><link>https://slds-lmu.github.io/i2ml/chapters/07_forests/</link><description>Recent content in Chapter 07: Random Forests on Introduction to Machine Learning (I2ML)</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://slds-lmu.github.io/i2ml/chapters/07_forests/index.xml" rel="self" type="application/rss+xml"/><item><title>Chapter 07.00: Random Forests: In a Nutshell</title><link>https://slds-lmu.github.io/i2ml/chapters/07_forests/07-00-nutshell-random-forest/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/07_forests/07-00-nutshell-random-forest/</guid><description>&lt;p>In this nutshell chunk, we delve into Random Forests, an ensemble method that harnesses multiple decision trees for improved prediction accuracy and robustness.&lt;/p></description></item><item><title>Chapter 07.01: Bagging Ensembles</title><link>https://slds-lmu.github.io/i2ml/chapters/07_forests/07-01-bagging/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/07_forests/07-01-bagging/</guid><description>&lt;p>Bagging (bootstrap aggregation) is a method for combining many models into a meta-model which often works much better than its individual components. In this section, we present the basic idea of bagging and explain why and when bagging works.&lt;/p></description></item><item><title>Chapter 07.02: Introduction</title><link>https://slds-lmu.github.io/i2ml/chapters/07_forests/07-02-intro/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/07_forests/07-02-intro/</guid><description>&lt;p>In this section we investigate random forests, a modification of bagging for trees. We illustrate the effect of ensemble size and show how to compute out-of-bag error estimates.&lt;/p></description></item><item><title>Chapter 07.03: Benchmarking Trees, Forests, and Bagging k-NN</title><link>https://slds-lmu.github.io/i2ml/chapters/07_forests/07-03-benchmark/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/07_forests/07-03-benchmark/</guid><description>&lt;p>We compare the performance of random forests vs. (bagged) CART and (bagged) \(k\)-NN.&lt;/p></description></item><item><title>Chapter 07.04: Feature Importance</title><link>https://slds-lmu.github.io/i2ml/chapters/07_forests/07-04-featureimportance/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/07_forests/07-04-featureimportance/</guid><description>&lt;p>In a complex machine learning model, the contributions of the different features to the model performance are difficult to evaluate. The concept of feature importance allows to quantify these effects for random forests.&lt;/p></description></item><item><title>Chapter 07.05: Proximities</title><link>https://slds-lmu.github.io/i2ml/chapters/07_forests/07-05-proximities/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/07_forests/07-05-proximities/</guid><description>&lt;p>The term &lt;em>proximity&lt;/em> refers to the &amp;ldquo;closeness&amp;rdquo; between pairs of cases. Proximities are calculated for each pair of observations and can be derived directly from random forests.&lt;/p></description></item><item><title>Chapter 07.06: Discussion</title><link>https://slds-lmu.github.io/i2ml/chapters/07_forests/07-06-discussion/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/07_forests/07-06-discussion/</guid><description>&lt;p>In this section we discuss the advantages and disadvantages of random forests and explain that all advantages of trees also apply here.&lt;/p></description></item></channel></rss>