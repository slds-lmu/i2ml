<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Chapter 06: Classification and Regression Trees (CART) on Introduction to Machine Learning (I2ML)</title><link>https://slds-lmu.github.io/website_i2ml_copy/chapters/06_trees/</link><description>Recent content in Chapter 06: Classification and Regression Trees (CART) on Introduction to Machine Learning (I2ML)</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://slds-lmu.github.io/website_i2ml_copy/chapters/06_trees/index.xml" rel="self" type="application/rss+xml"/><item><title>Chapter 06.01: Introduction</title><link>https://slds-lmu.github.io/website_i2ml_copy/chapters/06_trees/06-01-intro/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/website_i2ml_copy/chapters/06_trees/06-01-intro/</guid><description>&lt;p>Decision trees are an important type of machine learning model and come in two main types: classification and regression trees. In this section, we explain the general idea of CART and show how they recursively divide up the input space into ever smaller rectangular partitions.&lt;/p></description></item><item><title>Chapter 06.02: Splitting Criteria</title><link>https://slds-lmu.github.io/website_i2ml_copy/chapters/06_trees/06-02-splitcriteria/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/website_i2ml_copy/chapters/06_trees/06-02-splitcriteria/</guid><description>&lt;p>CART algorithms require splitting criteria for trees, which are usually defined in terms of impurity reduction. In this section we formalize the idea of splitting criteria and explain the details of splitting for both regression and classification.&lt;/p></description></item><item><title>Chapter 06.03: Growing a Tree</title><link>https://slds-lmu.github.io/website_i2ml_copy/chapters/06_trees/06-03-treegrowing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/website_i2ml_copy/chapters/06_trees/06-03-treegrowing/</guid><description>&lt;p>In this section, we explain how to grow a tree starting with an empty tree, i.e., a root node containing all the data. It will be shown that trees are grown by recursively applying greedy optimization to each node.&lt;/p></description></item><item><title>Chapter 06.04: Computational Aspects of Finding Splits</title><link>https://slds-lmu.github.io/website_i2ml_copy/chapters/06_trees/06-04-splitcomputation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/website_i2ml_copy/chapters/06_trees/06-04-splitcomputation/</guid><description>&lt;p>In this section, we explain the computational aspects of the node-splitting procedure, especially for nominal features. In addition, we illustrate how to deal with missing values.&lt;/p></description></item><item><title>Chapter 06.05: Stopping Criteria &amp; Pruning</title><link>https://slds-lmu.github.io/website_i2ml_copy/chapters/06_trees/06-05-stoppingpruning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/website_i2ml_copy/chapters/06_trees/06-05-stoppingpruning/</guid><description>&lt;p>The recursive partitioning procedure used to grow a CART usually leads to problems such as exponential growth of computations, overfitting, and the horizon effect. To deal with these problems, we can use stopping criteria and pruning. In this section, we explain the basis of these two solutions.&lt;/p></description></item><item><title>Chapter 06.06: Discussion</title><link>https://slds-lmu.github.io/website_i2ml_copy/chapters/06_trees/06-06-discussion/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/website_i2ml_copy/chapters/06_trees/06-06-discussion/</guid><description>&lt;p>In this section we discuss the advantages and disadvantages of CART and mention other tree methodologies.&lt;/p></description></item></channel></rss>