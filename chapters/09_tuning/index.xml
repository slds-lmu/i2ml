<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Chapter 09: Tuning on Introduction to Machine Learning (I2ML)</title><link>https://slds-lmu.github.io/i2ml/chapters/09_tuning/</link><description>Recent content in Chapter 09: Tuning on Introduction to Machine Learning (I2ML)</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://slds-lmu.github.io/i2ml/chapters/09_tuning/index.xml" rel="self" type="application/rss+xml"/><item><title>Chapter 09.01: Introduction</title><link>https://slds-lmu.github.io/i2ml/chapters/09_tuning/09-01-intro/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/09_tuning/09-01-intro/</guid><description>&lt;p>While model parameters are optimized during training, hyperparameters must be specified in advance. In this section, we will motivate why it is crucial to find good values for, i.e. to tune, these hyperparameters.&lt;/p></description></item><item><title>Chapter 09.02: Problem Definition</title><link>https://slds-lmu.github.io/i2ml/chapters/09_tuning/09-02-tuning-tuningproblem/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/09_tuning/09-02-tuning-tuningproblem/</guid><description>&lt;p>Hyperparameter tuning is the process of finding good model hyperparameters. In this section we formalize the problem of tuning and explain why tuning is computationally hard.&lt;/p></description></item><item><title>Chapter 09.03: Basic Techniques</title><link>https://slds-lmu.github.io/i2ml/chapters/09_tuning/09-03-basicalgos/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/09_tuning/09-03-basicalgos/</guid><description>&lt;p>In this section we familiarize ourselves with two simple but popular tuning strategies, namely grid search and random search, and discuss their advantages and disadvantages.&lt;/p></description></item><item><title>Chapter 09.04: Advanced Tuning Techniques</title><link>https://slds-lmu.github.io/i2ml/chapters/09_tuning/09-04-tuning-advanced/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/09_tuning/09-04-tuning-advanced/</guid><description>&lt;p>Besides grid search and random search there are several more advanced techniques for hyperparameter optimization. In this section we focus on model based optimization methods such as Bayesian optimization. Furthermore, we look into multi-fidelity methods such as the hyperband algorithm.&lt;/p></description></item><item><title>Chapter 09.05: Pipelines and AutoML</title><link>https://slds-lmu.github.io/i2ml/chapters/09_tuning/09-05-tuning-pipelines/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/09_tuning/09-05-tuning-pipelines/</guid><description>&lt;p>Some aspects of the machine learning lifecycle can be automated via
AutoML. In this section we look into pipelines as part of AutoML and how (HPO-) pipelines can be represented as directed acyclic graphs (DAGs).&lt;/p></description></item><item><title>Tuning: Further Material</title><link>https://slds-lmu.github.io/i2ml/chapters/09_tuning/further-material/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/09_tuning/further-material/</guid><description/></item></channel></rss>