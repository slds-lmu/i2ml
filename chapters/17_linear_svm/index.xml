<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Chapter 17: Linear Support Vector Machines on Introduction to Machine Learning (I2ML)</title><link>https://slds-lmu.github.io/website_i2ml_copy/chapters/17_linear_svm/</link><description>Recent content in Chapter 17: Linear Support Vector Machines on Introduction to Machine Learning (I2ML)</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://slds-lmu.github.io/website_i2ml_copy/chapters/17_linear_svm/index.xml" rel="self" type="application/rss+xml"/><item><title>Chapter 17.01: Linear Hard Margin SVM</title><link>https://slds-lmu.github.io/website_i2ml_copy/chapters/17_linear_svm/17-01-hard-margin/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/website_i2ml_copy/chapters/17_linear_svm/17-01-hard-margin/</guid><description>&lt;p>Hard margin SVMs seek perfect data separation. We introduce the linear hard margin SVM problem as a quadratic optimization program.&lt;/p></description></item><item><title>Chapter 17.02: Hard Margin SVM Dual</title><link>https://slds-lmu.github.io/website_i2ml_copy/chapters/17_linear_svm/17-02-hard-margin-dual/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/website_i2ml_copy/chapters/17_linear_svm/17-02-hard-margin-dual/</guid><description>&lt;p>In this section, we derive the dual variant of the linear hard-margin SVM problem, a computationally favorable formulation.&lt;/p></description></item><item><title>Chapter 17.03: Soft Margin SVM</title><link>https://slds-lmu.github.io/website_i2ml_copy/chapters/17_linear_svm/17-03-soft-margin/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/website_i2ml_copy/chapters/17_linear_svm/17-03-soft-margin/</guid><description>&lt;p>Hard margin SVMs are often not applicable to practical questions because they fail when the data are not linearly separable. Moreover, for the sake of generalization, we will often accept some violations to keep the margin large enough for robust class separation. Therefore, we introduce the soft margin linear SVM.&lt;/p></description></item><item><title>Chapter 17.04: SVMs and Empirical Risk Minimization</title><link>https://slds-lmu.github.io/website_i2ml_copy/chapters/17_linear_svm/17-04-erm/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/website_i2ml_copy/chapters/17_linear_svm/17-04-erm/</guid><description>&lt;p>In this section, we show how the SVM problem can be understood as an instance of empirical risk minimization.&lt;/p></description></item><item><title>Chapter 17.05: SVM Training</title><link>https://slds-lmu.github.io/website_i2ml_copy/chapters/17_linear_svm/17-05-optimization/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/website_i2ml_copy/chapters/17_linear_svm/17-05-optimization/</guid><description>&lt;p>The linear SVM problem is challenging due to its non-differentiability. In this section, we present methods of optimization.&lt;/p></description></item></channel></rss>