<!DOCTYPE html>
<html><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="/website_i2ml_copy/css/style.css">


<title>Introduction to Machine Learning (I2ML) | Chapter 04: Performance Evaluation</title>


<link rel="apple-touch-icon" sizes="180x180" href="/website_i2ml_copy/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/website_i2ml_copy/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/website_i2ml_copy/favicon-16x16.png">
<link rel="manifest" href="/website_i2ml_copy/site.webmanifest">
<link rel="mask-icon" href="/website_i2ml_copy/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">

</head><body>
<img id="logo" src="/website_i2ml_copy/i2ml.svg" />

<div id="nav-border" class="container">
    <nav id="nav" class="nav justify-content-center">
        
        <a class="nav-link" href="/website_i2ml_copy">
        
        Home
        </a>
        
        <a class="nav-link" href="/website_i2ml_copy/chapters/">
        
        Chapters
        </a>
        
        <a class="nav-link" href="/website_i2ml_copy/appendix/">
        
        Appendix
        </a>
        
        <a class="nav-link" href="/website_i2ml_copy/exercises/">
        
        Exercises
        </a>
        
        <a class="nav-link" href="/website_i2ml_copy/references/">
        
        References
        </a>
        
        <a class="nav-link" href="/website_i2ml_copy/team/">
        
        Team
        </a>
        
    </nav>
</div><div id="content" class="container">
<h1>Chapter 04: Performance Evaluation</h1>

<p><p>This chapter treats the challenge of evaluating the performance of a model. We will introduce different performance measures for regression and classification tasks, explain the problem of overfitting as well as the difference between training and test error, and, lastly, present a variety of resampling techniques.</p>
</p>


<div class="chapter_overview">
<ul class="list-unstyled">


<li>
    <a class="title" href="/website_i2ml_copy/chapters/04_evaluation/04-01-generalization-error/">Chapter 04.01: Generalization Error</a>
    
      
        <p>It is a crucial part of machine learning to evaluate the performance of a learner. We will explain the concept of generalization error and the difference between inner and outer loss.
</p>
      
      
</li>

<li>
    <a class="title" href="/website_i2ml_copy/chapters/04_evaluation/04-02-measures-regression/">Chapter 04.02: Measures Regression</a>
    
      
        <p>In this section we familiarize ourselves with essential performance measures for regression. In particular, mean squared error (MSE), mean absolute error (MAE), and a straightforward generalization of $R^2$ are discussed.
</p>
      
      
</li>

<li>
    <a class="title" href="/website_i2ml_copy/chapters/04_evaluation/04-03-train/">Chapter 04.03: Training Error</a>
    
      
        <p>There are two types of errors: training errors and test errors. The focus of this section is on the training error and related difficulties.
</p>
      
      
</li>

<li>
    <a class="title" href="/website_i2ml_copy/chapters/04_evaluation/04-04-test/">Chapter 04.04: Test Error</a>
    
      
        <p>While we can infer some information about the learning process from training errors (e.g., the state of iterative optimization), we are truly interested in generalization ability, and thus in the test error on previously unseen data.
</p>
      
      
</li>

<li>
    <a class="title" href="/website_i2ml_copy/chapters/04_evaluation/04-05-overfitting-underfitting/">Chapter 04.05: Overfitting &amp; Underfitting</a>
    
      
        <p>In machine learning, we are interested in a model that captures the true underlying function and still generalizes well to new data. When the model fails on the first task, we speak of underfitting, and both train and test error will be high. On the other hand, learning the training data very well at the expense of generalization ability is referred to as overfitting and usually occurs when there is not enough data to tell our hypotheses apart. We will show you examples of this behavior and how to diagnose overfitting.
</p>
      
      
</li>

<li>
    <a class="title" href="/website_i2ml_copy/chapters/04_evaluation/04-06-resampling-1/">Chapter 04.06: Resampling 1</a>
    
      
        <p>Different resampling techniques help to assess the performance of a learner while avoiding potential quirks resulting from a single train-test split. We will introduce cross-validation (with and without stratification), bootstrap and subsampling.
</p>
      
      
</li>

<li>
    <a class="title" href="/website_i2ml_copy/chapters/04_evaluation/04-07-resampling-2/">Chapter 04.07: Resampling 2</a>
    
      
        <p>We provide a deep dive on resampling, showing its superiority to holdout splitting and analyzing the bias-variance decomposition of its MSE. We further point out the dependence between CV fold results and that hypothesis testing is therefore not applicable, and give some practical tips to choose resampling strategies.
</p>
      
      
</li>

<li>
    <a class="title" href="/website_i2ml_copy/chapters/04_evaluation/04-08-measures-classification/">Chapter 04.08: Measures Classification</a>
    
      
        <p>Analogous to regression, we consider essential performance measures for classification. As a classifier predicts either class labels or scores/probabilities, its performance can be evaluated based on these two notions. We show some performance measures for classification, including misclassification error rate (MCE), accuracy (ACC) and Brier score (BS). In addition, we will see confusion matrices and learn about costs.
</p>
      
      
</li>

<li>
    <a class="title" href="/website_i2ml_copy/chapters/04_evaluation/04-09-rocbasics/">Chapter 04.09: ROC Basics</a>
    
      
        <p>From the confusion matrix we can calculate a variety of ROC metrics. Among others, we will explain true positive rate, negative predictive value and the $F1$ measure.
</p>
      
      
</li>

<li>
    <a class="title" href="/website_i2ml_copy/chapters/04_evaluation/04-10-roccurves/">Chapter 04.10: ROC Curves</a>
    
      
        <p>In this section, we explain the ROC curve and how to calculate it. In addition, we will present the AUC as a global performance measure that integrates over all possible thresholds.
</p>
      
      
</li>

<li>
    <a class="title" href="/website_i2ml_copy/chapters/04_evaluation/04-11-partialauc-mcauc/">Chapter 04.11: Partial AUC &amp; Multi-Class AUC</a>
    
      
        <p>We discuss both the partial AUC, which restricts the AUC to the relevant area for a specific application, and possible extensions of the AUC to multi-class classification.
</p>
      
      
</li>

<li>
    <a class="title" href="/website_i2ml_copy/chapters/04_evaluation/04-12-prcurves/">Chapter 04.12: Precision-Recall Curves</a>
    
      
        <p>Besides plotting TPR against FPR to obtain the ROC curve, it sometimes makes sense to instead consider precision (= PPV) vs recall (= TPR), especially when data are imbalanced.
</p>
      
      
</li>

<li>
    <a class="title" href="/website_i2ml_copy/chapters/04_evaluation/04-13-auc-mwu/">Chapter 04.13: AUC &amp; Mann-Whitney-U Test</a>
    
      
        <p>We demonstrate that the AUC is equivalent to the normalized test statistic in the Mann-Whitney-U test, both of which are effectively rank-based metrics.
</p>
      
      
</li>


</ul>
</div>


        </div><footer class="bg-light text-center text-lg-start fixed-bottom">
<ul class="list-inline text-center">
  <li class="list-inline-item">Â© 2021 Course Creator</li>
  
  <li class="list-inline-item"><a class="nav-link" href="https://github.com/slds-lmu/lecture_i2ml" target="_blank">Course content</a></li>
  
  <li class="list-inline-item"><a class="nav-link" href="/website_i2ml_copy" target="_blank">Main Course Website</a></li>
  
  <li class="list-inline-item"><a class="nav-link" href="https://github.com/slds-lmu/i2ml" target="_blank">Website source code</a></li>
  
</ul>
</footer>

<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      ignoreHtmlClass: ['quizdown']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</body>
</html>
