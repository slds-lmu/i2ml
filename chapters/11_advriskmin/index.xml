<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Chapter 11: Advanced Risk Minimization on Introduction to Machine Learning (I2ML)</title><link>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/</link><description>Recent content in Chapter 11: Advanced Risk Minimization on Introduction to Machine Learning (I2ML)</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/index.xml" rel="self" type="application/rss+xml"/><item><title>Chapter 11.01: Risk Minimizers</title><link>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-01-risk-minimizer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-01-risk-minimizer/</guid><description>&lt;p>We introduce important concepts in theoretical risk minimization: risk minimizer, Bayes risk, Bayes regret, consistent learners and the optimal constant model.&lt;/p></description></item><item><title>Chapter 11.02: Pseudo-Residuals</title><link>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-02-pseudo-residuals/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-02-pseudo-residuals/</guid><description>&lt;p>We introduce the concept of pseudo-residuals, i.e., loss residuals in function space, and discuss their relation to gradient descent.&lt;/p></description></item><item><title>Chapter 11.03: L2 Loss</title><link>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-03-regression-l2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-03-regression-l2/</guid><description>&lt;p>In this section, we revisit \(L2\) loss and derive its risk minimizer &amp;ndash; the conditional mean &amp;ndash; and optimal constant model &amp;ndash; the empirical mean of observed target values.&lt;/p></description></item><item><title>Chapter 11.04: L1 Loss</title><link>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-04-regression-l1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-04-regression-l1/</guid><description>&lt;p>In this section, we revisit \(L1\) loss and derive its risk minimizer &amp;ndash; the conditional median &amp;ndash; and optimal constant model &amp;ndash; the empirical median of observed target values.&lt;/p></description></item><item><title>Chapter 11.05: Advanced Regression Losses</title><link>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-05-regression-further-losses/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-05-regression-further-losses/</guid><description>&lt;p>In this section, we introduce and discuss the following advanced regression losses: Huber, log-cosh, Cauchy, log-barrier, epsilon-insensitive, and quantile loss.&lt;/p></description></item><item><title>Chapter 11.06: 0-1 Loss</title><link>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-06-classification-01/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-06-classification-01/</guid><description>&lt;p>In this section, we revisit the 0-1 loss and derive its risk minimizer .&lt;/p></description></item><item><title>Chapter 11.07: Bernoulli Loss</title><link>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-07-classification-bernoulli/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-07-classification-bernoulli/</guid><description>&lt;p>We study the Bernoulli loss and derive its risk minimizer and optimal constant model. We further discuss the connection between Bernoulli loss minimization and tree splitting according to the entropy criterion.&lt;/p></description></item><item><title>Chapter 11.08: Brier Score</title><link>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-08-classification-brier/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-08-classification-brier/</guid><description>&lt;p>In this section, we introduce the Brier score and derive its risk minimizer and optimal constant model. We further discuss the connection between Brier score minimization and tree splitting according to the Gini index.&lt;/p></description></item><item><title>Chapter 11.09: Advanced Classification Losses</title><link>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-09-classification-further-losses/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-09-classification-further-losses/</guid><description>&lt;p>In this section, we introduce and discuss the following advanced classification losses: (squared) hinge loss, \(L2\) loss on scores, exponential loss, and AUC loss.&lt;/p></description></item><item><title>Chapter 11.10: Maximum Likelihood Estimation vs Empirical Risk Minimization I</title><link>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-10-max-likelihood-l2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-10-max-likelihood-l2/</guid><description>&lt;p>We discuss the connection between maximum likelihood estimation and risk minimization, then demonstrate the correspondence between a Gaussian error distribution and \(L2\) loss.&lt;/p></description></item><item><title>Chapter 11.11: Maximum Likelihood Estimation vs Empirical Risk Minimization II</title><link>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-11-max-likelihood-other/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-11-max-likelihood-other/</guid><description>&lt;p>We discuss the connection between maximum likelihood estimation and risk minimization for further losses (\(L1\) loss, Bernoulli loss).&lt;/p></description></item></channel></rss>