<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Chapter 11: Advanced Risk Minimization on Introduction to Machine Learning (I2ML)</title><link>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/</link><description>Recent content in Chapter 11: Advanced Risk Minimization on Introduction to Machine Learning (I2ML)</description><generator>Hugo</generator><language>en-us</language><atom:link href="https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/index.xml" rel="self" type="application/rss+xml"/><item><title>Chapter 11.01: Risk Minimization Basics</title><link>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-01-risk-minimizer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-01-risk-minimizer/</guid><description>&lt;p>We introduce important concepts in theoretical risk minimization: risk minimizer, Bayes risk, Bayes regret, consistent learners and the optimal constant model.&lt;/p></description></item><item><title>Chapter 11.02: Properties of Loss Functions</title><link>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-02-loss-properties/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-02-loss-properties/</guid><description>&lt;p>We introduce key properties of loss functions and explore how these influence model assumptions, sensitivity to outliers, and the tractability of training.&lt;/p></description></item><item><title>Chapter 11.03: Pseudo-Residuals</title><link>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-03-pseudo-residuals/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-03-pseudo-residuals/</guid><description>&lt;p>We introduce the concept of pseudo-residuals, i.e., loss residuals in function space, and discuss their relation to gradient descent.&lt;/p></description></item><item><title>Chapter 11.04: Regression Losses: L2 and L1 loss</title><link>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-04-regression-l2-l1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-04-regression-l2-l1/</guid><description>&lt;p>In this section, we revisit L2 and L1 loss, highlighting that their risk minimizers are the conditional mean and median, respectively, and that their optimal constant models correspond to the empirical mean and median of observed targets.&lt;/p></description></item><item><title>Chapter 11.05: L1 loss: Deep Dive</title><link>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-05-l1-deep-dive/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-05-l1-deep-dive/</guid><description>&lt;p>In this &lt;strong>deep dive&lt;/strong>, we revisit \(L1\) loss and derive its risk minimizer &amp;ndash; the conditional median &amp;ndash; and optimal constant model &amp;ndash; the empirical median of observed target values. Please note that there are no videos accompanying this section.&lt;/p></description></item><item><title>Chapter 11.06: Advanced Regression Losses</title><link>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-06-regression-further-losses/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-06-regression-further-losses/</guid><description>&lt;p>In this section, we introduce and discuss the following advanced regression losses: Huber, log-cosh, Cauchy, epsilon-insensitive, and quantile loss.&lt;/p></description></item><item><title>Chapter 11.07: Classification and 0-1-Loss</title><link>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-07-classification-01/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-07-classification-01/</guid><description>&lt;p>In this section, we revisit the 0-1-loss and derive its risk minimizer.&lt;/p></description></item><item><title>Chapter 11.08: Bernoulli Loss</title><link>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-08-classification-bernoulli/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-08-classification-bernoulli/</guid><description>&lt;p>We study the Bernoulli loss and derive its risk minimizer and optimal constant model. We further discuss the connection between Bernoulli loss minimization and tree splitting according to the entropy criterion.&lt;/p></description></item><item><title>Chapter 11.09: Logistic Regression: Deep Dive</title><link>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-09-classification-logreg-deep-dive/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-09-classification-logreg-deep-dive/</guid><description>&lt;p>In this segment, we derive the gradient and Hessian of logistice regression and show that logistic regression is a convex problem. This section is presented as a &lt;strong>deep-dive&lt;/strong>. Please note that there are no videos accompanying this section.&lt;/p></description></item><item><title>Chapter 11.10: Proper Scoring Rules</title><link>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-10-proper-scoring-rules/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-10-proper-scoring-rules/</guid><description>&lt;p>We dive into how proper scoring rules - like log loss and the Brier score (unlike L1) - guarantee strictly proper, probability-calibrated predictions by satisfying a first‑order optimality condition.&lt;/p></description></item><item><title>Chapter 11.11: Brier Score - L2/L1 Loss on Probabilities</title><link>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-11-classification-brier-l1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-11-classification-brier-l1/</guid><description>&lt;p>In this section, we introduce the Brier score and derive its risk minimizer and optimal constant model.&lt;/p></description></item><item><title>Chapter 11.12: Loss functions and tree splitting: Deep Dive</title><link>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-12-tree-splitting-deep-dive/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-12-tree-splitting-deep-dive/</guid><description>&lt;p>Learn how minimizing Bernoulli (log) loss yields entropy‐based splits and minimizing the Brier score yields Gini‐based splits, unifying impurity and risk views for optimal tree splitting. This section is presented as a &lt;strong>deep-dive&lt;/strong>. Please note that there are no videos accompanying this section.&lt;/p></description></item><item><title>Chapter 11.13: Advanced Classification Losses</title><link>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-13-classification-further-losses/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-13-classification-further-losses/</guid><description>&lt;p>In this section, we introduce and discuss the following advanced classification losses: (squared) hinge loss, \(L2\) loss on scores, exponential loss, and AUC loss.&lt;/p></description></item><item><title>Chapter 11.14: Optimal constant model for the empirical log loss risk: Deep Dive</title><link>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-14-classification-deep-dive/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-14-classification-deep-dive/</guid><description>&lt;p>In this segment, we explore the derivation of the optimal constant model concerning the empirical log loss risk. This section is presented as a &lt;strong>deep-dive&lt;/strong>. Please note that there are no videos accompanying this section.&lt;/p></description></item><item><title>Chapter 11.15: Maximum Likelihood Estimation vs Empirical Risk Minimization</title><link>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-15-max-likelihood/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-15-max-likelihood/</guid><description>&lt;p>We discuss the connection between maximum likelihood estimation and risk minimization, then demonstrate the correspondence between a Gaussian error distribution and \(L2\) loss and that alternative likelihoods give rise to the \(L1\) and Bernoulli loss.&lt;/p></description></item><item><title>Chapter 11.16: Bias Variance Decomposition I</title><link>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-16-bias-variance-decomposition/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-16-bias-variance-decomposition/</guid><description>&lt;p>We discuss how to decompose the generalization error of a learner.&lt;/p></description></item><item><title>Chapter 11.17: Bias Variance Decomposition II</title><link>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-17-bias-variance-decomposition2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-17-bias-variance-decomposition2/</guid><description>&lt;p>We discuss how to decompose the excess risk into the estimation, approximation and optimization error.&lt;/p></description></item><item><title>Chapter 11.18: Bias Variance Decomposition: Deep Dive</title><link>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-18-bias-variance-deep-dive/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-18-bias-variance-deep-dive/</guid><description>&lt;p>In this segment, we discuss details of the decomposition of the generalization error of a learner. This section is presented as a &lt;strong>deep-dive&lt;/strong>. Please note that there are no videos accompanying this section.&lt;/p></description></item></channel></rss>